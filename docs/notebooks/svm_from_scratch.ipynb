{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (from scratch)\n",
    "\n",
    "Support vector machines are supervised machine learning models which can address the problems of both classification and regression."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUPPORT VECTOR CLASSIFIERS\n",
    "\n",
    "<img src=\"https://github.com/jellAIfish/fromscratchtoml/raw/master/docs/project/static/notebooks/images/1.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "$$ k = \\frac{w}{|w|}|k| $$\n",
    "$$ x = k + x' $$\n",
    "$$ x' = x - k$$\n",
    "\n",
    "our hypothesis equation -\n",
    "$$ f(x) = w.x + b $$\n",
    "Since the point `x'` lies on our hyperplane -\n",
    "$$ w.x' + b = 0 $$\n",
    "substituting `x'`\n",
    "$$ w.(x-k) + b = 0 $$\n",
    "$$ w.(x-\\frac{w}{|w|}|k|)+b = 0 $$\n",
    "$$ w.x - |w|k + b = 0 $$\n",
    "$$ k = \\frac{w.x + b}{|w|}$$\n",
    "\n",
    "If `k` is positive the point is at one side of the hyperplane and vica-versa but since we are concerned only about the magnitude we multiply `k` with the corresponding label `y`,\n",
    "\n",
    "$$\\gamma = y(\\frac{w.x + b}{|w|})$$\n",
    "\n",
    "This is called geomatric margin which is the euclidean distance between `x` and our hyperlane. It will always be positive since  $y \\in [1, -1]$.\n",
    "\n",
    "Our goal is to maximise the minimum geomatric margin.\n",
    " Mathematically we want to maximise $\\gamma$  w.r.t `w` and `b`-\n",
    "\n",
    "$$\\mathop {\\max }\\limits_{w,b} \\gamma$$\n",
    "\n",
    "$$\\gamma \\le y_i(\\frac{w.x_i + b}{|w|}) ,i \\in [1, m]$$\n",
    "such that it is the minimum geomatric margin amongst all the margins.\n",
    "\n",
    "Notice that our hypothesis function\n",
    "$$ f(x) = w.x + b $$\n",
    "wont change if we scale it up by some positive factor `t`\n",
    "$$ f_{new}(x) = t(w.x + b) $$\n",
    "\n",
    "Since for classifying purpose we only require the magnitude of our hypothesis function. This means that for any `x` we can scale $f(x)$ such that $f(x) = 1$.\n",
    "\n",
    "\n",
    "Therefore our min-max equation becomes.\n",
    "$$\\mathop {\\max }\\limits_{w,b} \\frac{1}{|w|}$$\n",
    "\n",
    "$$1 \\le y_i(\\frac{w.x_i + b}{|w|}) ,i \\in [1, m]$$\n",
    "\n",
    "Also maximising $\\frac{1}{|w|}$ will lead to the same result if we maximise $\\frac{|w|^2}{2}$. We do this because $\\frac{1}{|w|}$ is a non convex function.  \n",
    "<img src=\"https://github.com/jellAIfish/fromscratchtoml/raw/master/docs/project/static/notebooks/images/3.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "\n",
    "Our goal is to minimise\n",
    "$$\\ \\frac{|w|^2}{2}$$\n",
    "\n",
    "$$0 \\le y_i(\\frac{w.x_i + b}{|w|}) - 1 ,i \\in [1, m]$$\n",
    "\n",
    "\n",
    "## Lagrange equations\n",
    "If we want to minimise $f(x)$\n",
    "$$\\mathop {\\max} f(x)$$\n",
    "subject to\n",
    "$$g(x) = 0$$\n",
    "\n",
    "then the minimum is found when the derivatives of both the functions point in the same direction.\n",
    "$$\\nabla f(x) = \\alpha \\nabla g(x)$$\n",
    "$$\\nabla f(x) - \\alpha \\nabla g(x) = 0$$\n",
    "if we define a lagrange function such that -\n",
    "$$L(x,\\alpha ) = f(x) - \\alpha g(x)$$\n",
    "then -\n",
    "$${\\nabla}L(x,\\alpha ) = \\nabla f(x) - \\alpha \\nabla g(x)$$\n",
    "\n",
    "$$ L(x, \\alpha) = f(x) - \\sum\\limits_{i=1}^m\\alpha g(x)$$  \n",
    "\n",
    "where $\\alpha$ is lagrange multiplier.\n",
    "\n",
    "Using this for our minimisation problem.\n",
    "$$ L(w, b, \\alpha) = \\frac{|w|^2}{2} - \\sum\\limits_{i=1}^m\\alpha_i [ y_i(\\frac{w.x_i + b}{|w|}) - 1]$$  \n",
    "\n",
    "It's dual form is\n",
    "$$\\mathop {\\max }\\limits_\\alpha \\mathop {\\min }\\limits_{w,b}L(w,b,\\alpha )$$\n",
    "subject to\n",
    "$$\\alpha_i \\ge 0, i \\in [1, m]$$\n",
    "\n",
    "To minimise wrt to `w` and `b` -\n",
    "$${\\nabla_w}L = w - \\sum\\limits_{i=1}^m\\alpha_i y_i x_i = 0$$\n",
    "$${\\nabla_b}L = \\sum\\limits_{i=1}^m\\alpha_i y_i = 0$$\n",
    "\n",
    "substituting these values back in our lagrange equation.\n",
    "$$ L(\\alpha) = \\frac{|w|^2}{2} - \\sum\\limits_{j=1}^m\\alpha_j [ y_j(w.x_i + b) - 1]$$  \n",
    "$$ L(\\alpha) = \\frac{\\sum\\limits_{i=1}^m\\sum\\limits_{i=j}^m\\alpha_i y_i x_i\\alpha_j y_j x_j}{2} - \\sum\\limits_{j=1}^m\\alpha_j [ y_j(\\sum\\limits_{i=1}^m\\alpha_i y_i x_i.x_i + b) - 1]$$  \n",
    "\n",
    "$$ L(\\alpha) = \\frac{\\sum\\limits_{i=1}^m\\sum\\limits_{i=j}^m\\alpha_i y_i x_i\\alpha_j y_j x_j}{2} - \\sum\\limits_{i=1}^m\\sum\\limits_{i=1}^m\\alpha_j y_j(\\alpha_i y_i x_i.x_i) + \\sum\\limits_{j=1}^m\\alpha_j + b\\sum\\limits_{j=1}^m\\alpha_j y_j$$\n",
    "\n",
    " $$ L(\\alpha) = \\sum\\limits_{j=1}^m\\alpha_j -\\frac{\\sum\\limits_{i=1}^m\\sum\\limits_{i=j}^m\\alpha_i y_i x_i\\alpha_j y_j x_j}{2}  $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
